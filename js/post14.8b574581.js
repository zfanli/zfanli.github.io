(window["webpackJsonp"]=window["webpackJsonp"]||[]).push([["post14"],{bb7c:function(n,e){n.exports={attributes:{title:"Oracle 数据库 SQL 调优笔记 2 - 重现与实践",subtitle:"使用 Docker 快速搭建一个 Oracle 数据库测试环境，来做一次调优实战！",date:"2019-01-14T02:15:00.000Z",tags:["Oracle","SQL","Database","Docker"]},body:'\n上篇文章最后留了一个清除缓存的 Tips，这篇文章将会用到，所以在这里再贴一次。\n\n```sql\nalter system flush buffer_cache;\nalter system flush shared_pool;\n```\n\n---\n\n### 写在前面\n\n用两篇文章来对最近项目中遇到的 SQL 性能问题做一个笔记和总结。\n\n上篇文章中我们描述了优化的思路，并且结合案例介绍了几个主要问题的优化方案。这篇文章将搭建一个测试环境来对其中的一些案例进行重现，并套用解决方案来做一个实战。\n\n- [Oracle 数据库 SQL 调优笔记 1 - 思路和方案](#/post/Oracle%20数据库%20SQL%20调优笔记%201%20-%20思路和方案)\n- Oracle 数据库 SQL 调优笔记 2 - 重现与实践（本文）\n\n### 准备\n\n这是准备部分。\n\n#### 搭建测试环境\n\n> ⚠️ 如果你的环境已经准备就绪，可以跳过这一步。\n\n方便起见，直接用 Docker 来快速搭建一个 Oracle 数据库环境。如果你还没有 Docker 环境，那么首先从 [这里](https://www.docker.com/get-started) 下载并安装 Docker。\n\n在命令行确认一下 Docker 版本，确认是否安装成功。\n\n```bash\n$ docker --version\nDocker version 18.09.0, build 4d60db4\n```\n\n首先登陆 Docker Hub，使用下载时注册的账户。敲以下命令然后根据提示来登陆即可。\n\n```bash\n$ docker login\n```\n\n登陆完成后，可以拉取 Oracle 数据库环境了。使用下面的命令，拉取一个别人已经配置好的 Oracle 12c 版本的数据库。这个版本不是企业版，不能做表分区，不过这次我们不尝试表分区内容，所以标准版足够了。\n\n```bash\n$ docker pull sath89/oracle-12c\n```\n\n文件有点大，等安装完成后，使用下面的命令来启动 Oracle 数据库。\n\n```bash\n$ docker run -d -e WEB_CONSOLE=false -p 1521:1521 sath89/oracle-12c\n```\n\n稍等片刻，控制台会输出一长串的 ID，Oracle 数据库已经启动成功了，并且映射到本地 `1521` 端口上了，这时可以使用任何你喜欢的工具连接数据库进行后面的操作了。连接数据库时使用下面的参数。\n\n```yaml\nhostname: localhost\nport: 1521\nsid: xe\nservice name: xe\nusername: system\npassword: oracle\n```\n\n也可以直接进入虚拟机环境中使用 SQL\\*Plus。\n\n```bash\n# 首先拿到 Container 的 ID\n$ docker ps\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS                              NAMES\n2a00bcc34a0f        sath89/oracle-12c   "/entrypoint.sh "   8 minutes ago       Up 8 minutes        0.0.0.0:1521->1521/tcp, 8080/tcp   recursing_saha\n# 用拿到的 ID 进入控制台\n$ docker exec -it 2a00bcc34a0f /bin/bash\n# 可以看到前缀的变换，现在已经处于虚拟机中的控制台\n# 然后，切换到 oracle 用户\nroot@2a00bcc34a0f:/\\# su oracle\n# 进入 Oracle 安装目录\noracle@2a00bcc34a0f:/$ cd $ORACLE_HOME\n# 启动 sqlplus\noracle@2a00bcc34a0f:/u01/app/oracle/product/12.1.0/xe$ bin/sqlplus / as sysdba\n\nSQL*Plus: Release 12.1.0.2.0 Production on Tue Jan 8 09:51:44 2019\n\nCopyright (c) 1982, 2014, Oracle.  All rights reserved.\n\n\nConnected to:\nOracle Database 12c Standard Edition Release 12.1.0.2.0 - 64bit Production\n\nSQL> select 1 from dual;\n\n   1\n----------\n   1\n\nSQL>\n```\n\n要关闭 Oracle 数据库，输入下面的命令。如果不知道 Container ID，可以使用 `docker ps` 命令查询。\n\n```bash\n$ docker container stop [container id]\n```\n\n参考文档：\n\n- [Docker Documentation](https://docs.docker.com/)\n- [Docker Hub - sath89/oracle-12c](https://hub.docker.com/r/sath89/oracle-12c)\n\n#### 准备测试数据\n\n我们设计一个游戏场景。假设有两张表，一个叫做 `characters`，用来储存所有的角色信息，让我们一步一步创建。\n\n```sql\ncreate table characters_temp as select\n  -- 角色 ID（1-500000）递增，左边补零到 8 位\n  lpad(level, 8, \'0\') as character_id,\n  -- 玩家 ID，可重复（1-500000）中随机\n  floor(dbms_random.value(1, 500000)) as gamer_id,\n  -- 角色性别，0: Female，1: Male\n  floor(dbms_random.value(0, 2)) as character_gender,\n  -- 角色等级（1-100）中随机\n  floor(dbms_random.value(1, 100)) as character_level,\n  -- 角色持有的金币（1-10000）中随机\n  floor(dbms_random.value(1, 10000)) as character_coin,\n  -- 创建时间\n  current_timestamp as create_date,\n  -- 角色名称\n  \'NAME_\' || level as character_name\nfrom dual connect by level <= 500000;\n```\n\n执行上面的语句，可以得到一张 50 万数据的临时角色表。不过目前字段比较少，真实场景肯定不止这些字段，我们利用笛卡尔积来丰富测试数据，让两张表相乘，扩充一下其他字段作为背景数据。这里看上去 `all_indexes` 表字段比较多，就用它来扩充数据了。\n\n```sql\ncreate table characters as select\n  t.*, d.*\nfrom\n  characters_temp t,\n  (select * from all_indexes\n  where rownum = 1) d;\n```\n\n执行完上面的语句，我们的数据量虽然不变，但是字段数量增加了不少。我们先给这张表加上主键。\n\n```sql\nalter table characters\n  modify character_id varchar2(8) not null;\nalter table characters\n  add constraint characters_pk primary key (character_id);\n```\n\n到这里角色表的数据就准备完成了，此时表结构应该是这样的。\n\n```sql\nSQL> desc characters;\n Name                  Null?    Type\n --------------------- -------- ----------------------------\n CHARACTER_ID          NOT NULL VARCHAR2(8)\n GAMER_ID                       NUMBER\n CHARACTER_GENDER               NUMBER\n CHARACTER_LEVEL                NUMBER\n CHARACTER_COIN                 NUMBER\n CREATE_DATE                    TIMESTAMP(6) WITH TIME ZONE\n CHARACTER_NAME                 VARCHAR2(45)\n-- 以上是我们会用到的字段，以下是背景数据的字段，共 60 个字段\n OWNER                 NOT NULL VARCHAR2(128)\n INDEX_NAME            NOT NULL VARCHAR2(128)\n ...\n ...\n ...\n ORPHANED_ENTRIES               VARCHAR2(3)\n INDEXING                       VARCHAR2(7)\n```\n\n并且存在一个主键索引。\n\n```sql\nSQL> select index_name from user_indexes\nwhere table_name = upper(\'characters\');\n\nINDEX_NAME\n---------------\nCHARACTERS_PK\n```\n\n另一张表 `items` 用来储存角色的所持物品信息，我们定义每个角色有 10 个物品栏，每个物品栏可以放一种物品，数量在 1 和 99 之间，设定类似 MineCraft。\n\n```sql\ncreate table items_temp as select\n  -- 物品顺序（1-10）\n  i.item_order as item_order,\n  -- 角色 ID\n  c.character_id as character_id,\n  -- 物品 ID（1-50）中随机\n  floor(dbms_random.value(1, 51)) as item_id,\n  -- 物品数量（1-99）中随机\n  floor(dbms_random.value(1, 100)) as item_num,\n  -- 物品可用 flag（0-1）中随机\n  floor(dbms_random.value(0, 2)) as enable_flag,\n  -- 更新时间\n  current_timestamp as last_update_time,\n  -- 物品描述\n  \'DESCRIPTION_\' || current_timestamp as item_description\nfrom\n  characters c,\n  (select level as item_order\n    from dual connect by level <= 10) i;\n```\n\n为了方便起见，物品表将给每个角色都创建 10 条记录，用来分别储存每个物品格子的信息，而对于物品格子中是否存在物品，则使用 `enable_flag` 来控制。执行这条语句，稍等片刻，可以得到了一张 500 万数据的测试表。\n\n依照之前的方法，将物品表也添加上一些其他字段来丰富数据。\n\n```sql\ncreate table items as select\n  i.*, d.*\nfrom\n  items_temp i,\n  (select * from all_indexes\n  where rownum = 1) d;\n```\n\n同样的，给物品表加上主键。物品表使用角色 ID + 物品顺序字段作为联合主键。\n\n```sql\nalter table items\n  modify item_order number not null;\nalter table items\n  add constraint items_pk primary key (character_id, item_order);\n```\n\n物品表的表结构应该是这样的。\n\n```sql\nSQL> desc items;\n Name                   Null?    Type\n --------------------- -------- ----------------------------\n ITEM_ORDER            NOT NULL NUMBER\n CHARACTER_ID          NOT NULL VARCHAR2(8)\n ITEM_ID                        NUMBER\n ITEM_NUM                       NUMBER\n ENABLE_FLAG                    NUMBER\n LAST_UPDATE_TIME               TIMESTAMP(6) WITH TIME ZONE\n ITEM_DESCRIPTION               VARCHAR2(85)\n-- 以上是我们会用到的字段，以下是背景数据的字段，共 60 个字段\n OWNER                 NOT NULL VARCHAR2(128)\n INDEX_NAME            NOT NULL VARCHAR2(128)\n ...\n ...\n ...\n ORPHANED_ENTRIES               VARCHAR2(3)\n INDEXING                       VARCHAR2(7)\n```\n\n还有一个主键索引。\n\n```sql\nSQL> select index_name from user_indexes\nwhere table_name = upper(\'items\');\n\nINDEX_NAME\n-----------\nITEMS_PK\n```\n\n测试数据的准备就完成了。\n\n#### 开始实战之前\n\n准备已经就绪，不过在进入实战之前，还有一些 Tips 可以帮助我们进展的更顺利。\n\n**打开 `timing` 显示执行时间。**\n\n`timing` 设置会把 SQL 执行的时间输出到控制台，方便我们评估性能，所以，你不用再掐表算时间了。\n\n```sql\nSQL> set timing on;\nSQL> select 1 from dual;\n\n   1\n----------\n   1\n\nElapsed: 00:00:00.02\n```\n\n**打开 `autotrace` 显示执行计划。**\n\n`autotrace` 设置可以让我们查看当前 SQL 的真实执行计划，来帮助我们定位性能问题所在。\n\n```sql\nSQL> set autotrace on;\nSQL> select 1 from dual;\n\n   1\n----------\n   1\n\nExecution Plan\n----------------------------------------------------------\nPlan hash value: 1388734953\n\n-----------------------------------------------------------------\n| Id  | Operation        | Name | Rows  | Cost (%CPU)| Time     |\n-----------------------------------------------------------------\n|   0 | SELECT STATEMENT |      |     1 |     2   (0)| 00:00:01 |\n|   1 |  FAST DUAL       |      |     1 |     2   (0)| 00:00:01 |\n-----------------------------------------------------------------\n\n\nStatistics\n----------------------------------------------------------\n    0  recursive calls\n    0  db block gets\n    0  consistent gets\n    0  physical reads\n    0  redo size\n  535  bytes sent via SQL*Net to client\n  551  bytes received via SQL*Net from client\n    2  SQL*Net roundtrips to/from client\n    0  sorts (memory)\n    0  sorts (disk)\n    1  rows processed\n```\n\n执行计划的解读可以参考下面的资料，但是官方资料很多地方都不在我们的关注点上，不过好在仔细阅读执行计划也能悟出很多有用的信息，至少走没走索引是看一眼就知道的。\n\n**进行表分析**\n\nOracle 的优化器根据表的统计信息来选择执行计划。\n\n有时我们会发现执行计划可能不太准确，比如添加一个索引后，查看执行计划发现 cost 降低了很多，但是实际执行起来却效率低下，这是为什么？\n\n很有可能是因为这张表没有统计信息，或者统计信息过时了，此时需要再次对表进行分析，更新统计信息，才能让优化器选择最合适的执行计划。\n\n执行下面的语句，对两张测试表进行分析。一般对表进行大规模对数据删除或者插入之后需要重新进行分析，来保证优化器选择正确的执行计划。\n\n```sql\nanalyze table characters compute statistics;\nanalyze table items compute statistics;\n```\n\n**⚠️ 仅测试环境下，降低服务器可用资源。**\n\n为了方便再现性能问题，我们可以限制一下 Docker 引擎使用的 CPU 资源，模拟低配置环境，参考下图。\n\n![Docker Configuration](/img/DockerConfiguration.png)\n\n参考文档：\n\n- [Database SQL Tuning Guide - 7 Reading Execution Plans](https://docs.oracle.com/database/121/TGSQL/tgsql_interp.htm#TGSQL94618)\n\n### 探索测试数据\n\n环境搭建好了，数据也准备好了，可以开始实战了。不过先别慌，这些数据都是随机生成的，我们不妨先来探索一下，了解这些数据的大致情况。\n\n> ⚠️ 由于大部分都是随机数据，所以如果你跟着我的步骤探索的话，你的结果可能会和我的不同。不妨来看看我们的数据之间有多大的差别？\n\n先来看看数据量，按照之前的 DDL 来看，`characters` 表应该有 50 万数据。\n\n```sql\nSQL> select count(1) from characters;\n\n  COUNT(1)\n----------\n    500000\n\nElapsed: 00:00:00.35\n```\n\n`items` 表则是 500 万。\n\n```sql\nSQL> select count(1) from items;\n\n  COUNT(1)\n----------\n   5000000\n\nElapsed: 00:00:00.58\n```\n\n嗯，看上去没问题，由于存在主键，`count(1)` 函数会使用到主键索引，所以速度会很快。从执行计划可以看出来。\n\n```sql\nSQL> set autotrace traceonly;\nSQL> select count(1) from items;\n\nElapsed: 00:00:00.18\n\nExecution Plan\n----------------------------------------------------------\nPlan hash value: 3881446812\n\n------------------------------------------------------------------------\n\n| Id  | Operation             | Name     | Rows  | Cost (%CPU)| Time    |\n\n------------------------------------------------------------------------\n\n|   0 | SELECT STATEMENT      |          |     1 |  4185   (1)| 00:00:01|\n\n|   1 |  SORT AGGREGATE       |          |     1 |            |         |\n\n|   2 |   INDEX FAST FULL SCAN| ITEMS_PK |  4523K|  4185   (1)| 00:00:01|\n\n------------------------------------------------------------------------\n\n\nNote\n-----\n   - dynamic statistics used: dynamic sampling (level=2)\n\n\nStatistics\n----------------------------------------------------------\n    0  recursive calls\n    0  db block gets\n15394  consistent gets\n    0  physical reads\n    0  redo size\n  542  bytes sent via SQL*Net to client\n  551  bytes received via SQL*Net from client\n    2  SQL*Net roundtrips to/from client\n    0  sorts (memory)\n    0  sorts (disk)\n    1  rows processed\n```\n\n请注意执行计划中的关键字 `ITEMS_PK` 和 `INDEX FAST FULL SCAN`，这说明这条 SQL 语句使用了主键索引，并且执行了索引快速全表扫描。如果这里没有主键会如何呢？我们尝试一下先删除主键，然后再执行一次之前的语句。\n\n```sql\nSQL> alter table items drop constraint items_pk;\n\nTable altered.\n\nElapsed: 00:00:00.31\n\nSQL> select count(1) from items;\n\n  COUNT(1)\n----------\n   5000000\n\nElapsed: 00:00:07.03\n```\n\n可以看到查询时间从不到 0.58 秒飙升到了 7.03 秒。主键索引让查询速度提升了十多倍，并且通常来说，数据了越大的情况下，提升的越明显。\n\n执行计划也不同，由于过于冗长，我就不贴具体的内容了，想看具体执行计划可以本地跑一下这条语句。\n\n如同之前一样，我们只需要关注几个关键字就足够了。没有主键的情况下 `items` 走了全表，执行了 `TABLE ACCESS FULL` 计划，从字面上理解，就是全表检索。并且 cost 为 58940，而使用主键的情况下 cost 为 4185。\n\n⚠️ 如果你跟着我的步骤把主键删除了的话，别忘了用下面的语句再加回来。\n\n```sql\nalter table items\n  add constraint items_pk primary key (character_id, item_order);\n```\n\n下一步，我们来看看表空间有多大。\n\n```sql\nSQL> select\n  segment_name as table_name,\n  bytes / 1024 / 1024 mb\nfrom user_segments\nwhere segment_name = upper(\'characters\')\nor segment_name = upper(\'items\');\n\nTABLE_NAME        MB\n------------- ------\nCHARACTERS       151\nITEMS           1735\n\nElapsed: 00:00:00.03\n```\n\n接近 10 倍差距，正常范围。\n\n创建角色表的时候，玩家 ID 是使用一个 1-50,0000 之间的随机数来表示的，50 万的数据量下必定会存在重复的玩家 ID，对于重复的数据我们可以定义为同一个玩家所创建的不同角色。来让我们看看一共有多少个“独立”玩家。\n\n```sql\nSQL> select count(1) from (\n  select distinct gamer_id from characters\n);\n\n  COUNT(1)\n----------\n    316260\n\nElapsed: 00:00:01.04\n```\n\n从结果中我们得到了 2 个信息。最明显的信息告诉我们，一共存在超过 31 万的玩家，这说明有接近 19 万的角色是属于其他玩家下面的重复角色。另一个信息就是执行速度，超过了 1 秒。\n\n不过角色表规模较小，还看不出来这个 1 秒的差距。我们来看看数据规模大一点的物品表。\n\n物品表中有一个 `enable_flag` 来控制这个物品是否是有效的，所以来看看有效的物品一共有多少吧。\n\n```sql\nSQL> select count(1) from items where enable_flag = 1;\n\n  COUNT(1)\n----------\n   2499255\n\nElapsed: 00:00:06.33\n```\n\n一个非常趋于概率的结果。注意查询时间是 6 秒，对这样一个查询来说，这算是很长的时间了。我们已经可以看到一些性能问题的端倪。\n\n再来一个稍微复杂点的例子，来看看每个角色平均持有的有效物品数量。\n\n```sql\nSQL> select avg(c) from (\n  select count(1) c from items where enable_flag = 1\n  group by character_id\n);\n\n    AVG(C)\n----------\n5.00361369\n\nElapsed: 00:00:09.06\n```\n\n执行时间 9 秒！\n\n看看有多少角色拥有超过 5 个有效物品。\n\n```sql\nSQL> select count(c) from (\n  select character_id c from items where enable_flag = 1\n  group by character_id having count(1) > 5\n);\n\n  COUNT(C)\n----------\n    188762\n\nElapsed: 00:00:05.29\n```\n\n接近 19 万，执行了 5 秒。\n\n看看有多少角色等级超过 60 级。\n\n```sql\nSQL> select count(1) from characters where character_level > 60;\n\n  COUNT(1)\n----------\n    196683\n\nElapsed: 00:00:01.58\n```\n\n接近 20 万。这次查询的是规模小的角色表，执行时间 1 秒。\n\n我们已经大概了解了数据的情况。\n\n- 角色表只有 50 万数据，表空间仅 151 M，即使不使用索引，把全表数据加载到内存处理依旧很快；\n- 而物品表有 500 万数据，表空间有 1,735 M，任何走全表的操作都会降低 SQL 执行的速度，这个尺寸的数据已经不太适合全部加载到内存后再做操作了。\n\n### 重现索引优化案例\n\n目前除了主键以外还没有其他索引，在之前的探索中我们已经发现了一些性能问题的端倪，现在让我们设计一个需求。\n\n角色表中的 `character_coin` 字段表示这个角色的持有金币数量，我们假设一个需求，出于统计目的，现在需要知道所有持有金币数量少于 `1,000` 的角色，其所持有的物品 ID 为 `7` 的物品的数量总和。\n\n- **需求**：\n  - 物品数量总和\n- **条件**：\n  - 角色持有金币数量少于 `1,000`\n  - 物品 ID 为 `7`\n  - 物品有效\n\n别去考虑这个需求有什么用。首先这是简单的 `inner join` 关系，条件也很明确，所以我们很快可以得出下面的 SQL 语句。\n\n```sql\nselect\n  sum(i.item_num)\nfrom\n  items i\ninner join\n  characters c\non\n  c.character_id = i.character_id\nwhere\n      c.character_coin <= 1000\n  and i.enable_flag = 1\n  and i.item_id = 7;\n```\n\n把这条语句丢给 Oracle 执行一下试试看。\n\n```sql\nSUM(I.ITEM_NUM)\n---------------\n         256808\n\nElapsed: 00:00:17.30\n```\n\n这个结果是物品表 `item_num` 字段的求和的值，虽然结果是 25 万多，但是涉及到的数据应该不会很多。最终这条语句执行花了 17.3 秒。\n\n让我们先来看看执行计划，尝试定位一下性能问题所在。\n\n```sql\nExecution Plan\n----------------------------------------------------------\nPlan hash value: 2594570633\n\n------------------------------------------------------------------------\n\n| Id  | Operation           | Name       | Rows  | Bytes |TempSpc| Cost(%CPU) | Time      |\n\n------------------------------------------------------------------------\n\n|   0 | SELECT STATEMENT    |            |     1 |    29 |       | 64304   (1)| 00:00:03 |\n\n|   1 |  SORT AGGREGATE     |            |     1 |    29 |       |            |          |\n\n|*  2 |   HASH JOIN         |            | 49985 |  1415K|  1128K| 64304   (1)| 00:00:03 |\n\n|*  3 |    TABLE ACCESS FULL| CHARACTERS | 50010 |   537K|       |  5218   (1)| 00:00:01 |\n\n|*  4 |    TABLE ACCESS FULL| ITEMS      | 49985 |   878K|       | 58960   (1)| 00:00:03 |\n\n------------------------------------------------------------------------\n\n\nPredicate Information (identified by operation id):\n---------------------------------------------------\n\n   2 - access("C"."CHARACTER_ID"="I"."CHARACTER_ID")\n   3 - filter("C"."CHARACTER_COIN"<=1000)\n   4 - filter("I"."ITEM_ID"=7 AND "I"."ENABLE_FLAG"=1)\n\n\nStatistics\n----------------------------------------------------------\n    191  recursive calls\n      0  db block gets\n 238878  consistent gets\n 237747  physical reads\n      0  redo size\n    551  bytes sent via SQL*Net to client\n    551  bytes received via SQL*Net from client\n      2  SQL*Net roundtrips to/from client\n      7  sorts (memory)\n      0  sorts (disk)\n      1  rows processed\n```\n\n执行计划提示了很多信息，我们提取一些对我们有用的出来。\n\n- 整体 cost 为 `64,304`\n- `CHARACTERS` 访问了全表\n- `ITEMS` 访问了全表\n\n#### 开始尝试添加索引\n\n从执行计划中可以了解到，两张表都走了全表检索是真正影响执行效率的原因所在，对于这个场合我们不需要所有的数据，所以可以使用索引来优化查询性能。\n\n对于角色表来说，仅使用到两个字段作为查询的条件，`character_id` 作为关联条件，`character_coin` 作为筛选条件。所以对这 2 个字段创建索引。\n\n```sql\ncreate index characters_index1 on characters (\n  character_id,\n  character_coin\n);\n```\n\n对物品表来说，有 3 个字段作为查询条件，一个 `character_id` 作为关联条件，然后 `enable_flag` 和 `item_id` 作为筛选条件；此外，还有 1 个字段 `item_num` 最终被选出进行求和，所以一共需要对 4 个字段创建索引。\n\n```sql\ncreate index items_index1 on items (\n  character_id,\n  enable_flag,\n  item_id,\n  item_num\n);\n```\n\n创建好上面的索引，我们准备再执行一次之前的语句。不过先别急，Oracle 会自动使用缓存，所以如果重复查询同一条 SQL，速度只会越来越快，这会影响我们尝试性能调优的结果，所以我们先执行下面两条语句，清除掉缓存的影响。\n\n```sql\nalter system flush buffer_cache;\nalter system flush shared_pool;\n```\n\n好了，接下来让我们重新执行一次之前的语句。\n\n```sql\nSQL> select\n  sum(i.item_num)\nfrom\n  items i\ninner join\n  characters c\non\n  c.character_id = i.character_id\nwhere\n      c.character_coin <= 1000\n  and i.enable_flag = 1\n  and i.item_id = 7;\n\nSUM(I.ITEM_NUM)\n---------------\n         256808\n\nElapsed: 00:00:03.29\n```\n\n从 17.3 秒降低到了 3.29 秒，接近 5 倍的性能提升。再来看看执行计划上的变化。\n\n```sql\nExecution Plan\n----------------------------------------------------------\nPlan hash value: 3200269783\n\n------------------------------------------------------------------------\n\n| Id  | Operation              | Name              | Rows  | Bytes |TempSpc| Cost (%CPU)| Time     |\n\n------------------------------------------------------------------------\n\n|   0 | SELECT STATEMENT       |                   |   1   |    29 |       |  6000   (1)| 00:00:01 |\n\n|   1 |  SORT AGGREGATE        |                   |   1   |    29 |       |            |          |\n\n|*  2 |   HASH JOIN            |                   |  5337 |   151K|  1128K|  6000   (1)| 00:00:01 |\n\n|*  3 |    INDEX FAST FULL SCAN| CHARACTERS_INDEX1 | 50010 |   537K|       |   458   (1)| 00:00:01 |\n\n|*  4 |    INDEX FAST FULL SCAN| ITEMS_INDEX1      | 49985 |   878K|       |  5416   (1)| 00:00:01 |\n\n------------------------------------------------------------------------\n\n\nPredicate Information (identified by operation id):\n---------------------------------------------------\n\n   2 - access("C"."CHARACTER_ID"="I"."CHARACTER_ID")\n   3 - filter("C"."CHARACTER_COIN"<=1000)\n   4 - filter("I"."ITEM_ID"=7 AND "I"."ENABLE_FLAG"=1)\n\nNote\n-----\n   - dynamic statistics used: dynamic sampling (level=2)\n\n\nStatistics\n----------------------------------------------------------\n 1653  recursive calls\n    0  db block gets\n27292  consistent gets\n22596  physical reads\n    0  redo size\n  551  bytes sent via SQL*Net to client\n  551  bytes received via SQL*Net from client\n    2  SQL*Net roundtrips to/from client\n  106  sorts (memory)\n    0  sorts (disk)\n    1  rows processed\n```\n\n依旧从执行计划上提取我们感兴趣的信息。\n\n- 整体 cost 为 `6,000`\n  - 相较之前的 `64,304` 有明显的改善\n- 角色表使用了新建的 `CHARACTERS_INDEX1` 索引\n  - 相较于全表检索，全索引检索对性能有明显的改善\n- 物品表使用了新建的 `ITEMS_INDEX1` 索引\n  - 同上\n\n#### 小结\n\n我们对目前遇到的问题和使用的方法做一个小结。\n\n- 执行计划可以帮助我们定位问题的所在\n  - **前提是表进行过分析**\n  - 当执行计划不准确时可能是由于统计信息过时\n  - 你需要重新进行表分析\n- 创建索引时尽量包含查询字段\n  - \\*包含更多的字段将造成索引表所占空间增大\n  - \\*包含更多字段也将造成索引表检索时间变长\n  - 包含查询字段可以避免回查开销\n- 创建索引时如果仅包含查询条件\n  - 使用 `ROWID` 对原表进行回查获取查询字段的数据\n- 创建索引将降低数据写入效率\n  - 因为数据变动时需要同步更新索引表\n  - 写入操作多的表慎加索引\n\n### 重现 `merge` 优化案例\n\n就数据库的增删改查来说，可能我们平时熟悉的是 `insert`、`delete`、`update` 和 `select`，而 `merge` 是一个聚合操作，可以在一条语句里面实现增删改查。\n\nmerge 的语法结构表明了其适用于参照一张表的数据来操作另一张表的场合。其他操作暂且不论，当使用 merge 来替代 update 时，由于更新机制的不同，似乎 merge 拥有天然优势，经常能比 update 操作要快。\n\n下面的语句将物品表所有数据的 `enable_flag` 设置为 `0`，即失效所有物品。经过多次清除缓存再执行之后，在我本地环境下平均水平在 2 分 10 秒左右。\n\n```sql\nSQL> update items set enable_flag = 0;\n\n5000000 rows updated.\n\nElapsed: 00:02:16.88\n```\n\n下面则是使用 merge 来进行这一操作，同样的多次清除缓存执行的结果，平均水平在 1 分 30 秒左右。\n\n```sql\nSQL> merge into items i using dual\non (1=1) when matched then\nupdate set i.enable_flag = 0;\n\n5000000 rows merged.\n\nElapsed: 00:01:35.50\n```\n\n即使是这么简单的任务，merge 依旧比 update 要快 40 秒左右，提升 30% 的效率。\n\n但是通过分析两边的执行计划，我们得到了下面的信息。\n\n- `update` 的 cost 为 39,811；`merge` 为 59,127\n- `update` 访问并修改了 61M 数据；`merge` 最终修改了 61M 数据，但是中途访问了 1,239M 数据\n\nmerge 虽然速度快于 update，但是由于访问数据量远超过 update，所以优化器判定其开销大于 update。\n\n简单说，就是这种情况下 merge 会使用更多的内存来提升执行效率。不过 merge 本身并不是被设计成主要用来处理这种情况的，来看一个复杂点的例子。\n\n首先还是设计一个更新的需求。\n\n探索数据的时候我们发现同一个玩家 ID 下面会有重复的角色，而且重复角色有接近 19 万之多！现在要发布上一次活动的奖励，那就是增加每个符合条件的玩家的角色金币数量，就加一个金币好了，当然对于拥有多个角色的玩家来说，只有等级最高的那个角色才会有奖励。\n\n分析一下这个需求，我们需要做这些事情。\n\n- **需求**：\n  - 更新金币数量为原有数量 +1\n- **条件**：\n  - 一个玩家 ID 下面仅一个角色进行更新\n  - 玩家 ID 相同时更新等级最大的角色\n\n```sql\nupdate characters c set character_coin = (\n  select character_coin + 1\n  from (\n    select character_coin,\n      character_id,\n      row_number() over(\n        partition by gamer_id\n        order by character_level desc\n      ) as flag\n    from characters\n  ) t\n   where c.character_id = t.character_id\n   and flag = 1\n) where exists (\n  select 1 from (\n        select character_coin,\n      character_id,\n      row_number() over(\n        partition by gamer_id\n        order by character_level desc\n      ) as flag\n    from characters\n  ) t\n   where c.character_id = t.character_id\n   and flag = 1\n);\n```\n\n```sql\nmerge into characters c using (\n  select character_coin + 1 as character_coin\n  from (\n    select character_coin,\n      character_id,\n      row_number() over(\n        partition by gamer_id\n        order by character_level desc\n      ) as flag\n    from characters\n  ) t\n   where c.character_id = t.character_id\n) t on (\n  c.character_id = t.character_id\n  and t.flag = 1\n)\nwhen matched then\nupdate c.character_coin = t.character_coin;\n```\n\n// 施工现场\n\n机器性能限制没跑出来结果，时间原因无法继续调试下去，结论是这种情况下，`merge` 相较于 `update` 会快很多。原因是前者仅进行一次遍历，而后者由于结构问题会多次遍历原表，影响性能。\n\n等有时间并且有精力的时候把执行结果补上。\n',frontmatter:"title: Oracle 数据库 SQL 调优笔记 2 - 重现与实践\nsubtitle: 使用 Docker 快速搭建一个 Oracle 数据库测试环境，来做一次调优实战！\ndate: 2019-01-14 10:15:00 +8\n# lastUpdateTime: 2019-01-08 17:57:00 +8\ntags:\n  - Oracle\n  - SQL\n  - Database\n  - Docker"}}}]);
//# sourceMappingURL=post14.8b574581.js.map